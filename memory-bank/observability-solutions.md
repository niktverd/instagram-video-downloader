# Идеи для решения проблемы наблюдаемости

1. **Единая таблица статусов (VideoProcessStatus)**

   - Ввести отдельную таблицу (или коллекцию), где для каждого видео хранится текущий статус (enum: received, downloading, downloaded, processing, processed, publishing, published, error и т.д.), ссылка на связанные сущности и история переходов.
   - Все этапы пайплайна обновляют статус централизованно.

2. **Event Sourcing / Лог событий**

   - Вместо (или вместе с) статусом — писать в отдельную таблицу/коллекцию события: "скачивание начато", "скачивание завершено", "ошибка скачивания", "отправлено в pubsub", "cloud-run завершён" и т.д.
   - Позволяет строить таймлайны, видеть все переходы и причины ошибок.

3. **Дашборд мониторинга**

   - Сделать простую админку/дашборд (или даже Google Data Studio/Grafana), где визуализируются статусы, ошибки, "живые" процессы, тайминги этапов.
   - Можно строить поверх таблицы статусов или event log.

4. **Централизованный лог ошибок и алерты**

   - Все ошибки с этапов пайплайна (скачивание, cloud-run, публикация) логировать в одну таблицу/коллекцию с привязкой к видео и этапу.
   - Добавить алерты (например, через Telegram/Slack/email) при критических ошибках или зависших процессах.

5. **Трассировка процессов (Trace ID)**

   - Для каждого видео генерировать traceId, который прокидывается через все этапы (webhook → pubsub → cloud-run → публикация).
   - Все логи, статусы и ошибки помечаются этим traceId, что позволяет быстро находить всю историю по одному видео.

6. **Cloud Monitoring + Logging + Trace**

   - Использовать Google Cloud Monitoring для сбора кастомных метрик (кол-во задач, ошибки, длительность, queue backlog).
   - Cloud Logging для агрегации логов, фильтрации по traceId/correlationId.
   - Cloud Trace для распределённого трейсинга (цепочка вызовов по пайплайну).
   - Везде прокидывать traceId (accountId-scenarioId-sourceId).

7. **Dead-letter queue (DLQ) для PubSub**

   - Включить DLQ для задач, которые не удалось обработать после N ретраев.
   - Алерты на появление задач в DLQ, отдельный воркер для анализа и ручного вмешательства.

8. **Guardrails и лимиты**

   - Ограничить максимальное время выполнения задач (Cloud Run timeout), лимиты по CPU/RAM, concurrency.
   - В коде — таймауты на внешние вызовы, circuit breaker (например, через p-limit/bottleneck).

9. **Возможность отмены задач**

   - Хранить статус задач в БД, добавить флаг cancelled, воркеры периодически проверяют статус перед выполнением.
   - API для ручной отмены задач через админку.

10. **Визуализация очереди и статусов**

    - Дашборд (Grafana, Google Data Studio, Metabase) поверх таблицы статусов/event log.
    - Визуализация очереди, in-progress, ошибок, таймингов, стоимости.

11. **Idempotency everywhere**

    - Все этапы пайплайна должны быть идемпотентны (по уникальному ключу, не создавать дубликаты).

12. **Интеграция с алертингом**

    - Slack/Telegram/email интеграция для критичных алертов (через Cloud Monitoring или сторонние сервисы).

13. **Документация и runbooks**

    - Документировать типовые сбои и действия для поддержки (runbook), чтобы ускорить реакцию на инциденты.

14. **Таблица cloud-run-scenario-executions**
    - Хранить для каждой задачи: messageId, accountId, scenarioId, sourceId, status (enum: in-progress, success, fail, cancelled, timeout и т.д.), reqId, attempt, queue name, timestamps (created, started, finished), traceId/correlationId, error details, путь к артефакту (если есть).
    - Каждая попытка (attempt) — отдельная запись (или версия), чтобы видеть ретраи и их исход.
    - Использовать для мониторинга, алертов, дебага, визуализации очереди и истории выполнения.
    - Можно расширять: duration, cancelled flag, ссылки на связанные сущности, userId, etc.
